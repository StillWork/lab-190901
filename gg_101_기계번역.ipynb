{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hh_기계번역.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Oawi-8To_Z-p","colab_type":"code","outputId":"5600ff38-0a8e-43d6-e836-9d5972e1a1a7","executionInfo":{"status":"ok","timestamp":1565159891797,"user_tz":-540,"elapsed":3144,"user":{"displayName":"김화종","photoUrl":"https://lh4.googleusercontent.com/-sR-MVK_KsAg/AAAAAAAAAAI/AAAAAAAALbE/3e3Eq9nrGuw/s64/photo.jpg","userId":"17353049580175403985"}},"colab":{"base_uri":"https://localhost:8080/","height":91}},"source":["!curl http://www.manythings.org/anki/kor-eng.zip -o kor-eng.zip"],"execution_count":1,"outputs":[{"output_type":"stream","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 25348  100 25348    0     0  83656      0 --:--:-- --:--:-- --:--:-- 83656\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"61DXTqd3AaZW","colab_type":"code","outputId":"b3be939d-4ed6-4463-f061-6fcc95e9cc6d","executionInfo":{"status":"ok","timestamp":1565159897125,"user_tz":-540,"elapsed":3412,"user":{"displayName":"김화종","photoUrl":"https://lh4.googleusercontent.com/-sR-MVK_KsAg/AAAAAAAAAAI/AAAAAAAALbE/3e3Eq9nrGuw/s64/photo.jpg","userId":"17353049580175403985"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["!ls"],"execution_count":2,"outputs":[{"output_type":"stream","text":["kor-eng.zip  sample_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xTrhWOA-_h5u","colab_type":"code","outputId":"c2a181e1-e5cd-4054-f506-242d85a1c3ee","executionInfo":{"status":"ok","timestamp":1565159902740,"user_tz":-540,"elapsed":3385,"user":{"displayName":"김화종","photoUrl":"https://lh4.googleusercontent.com/-sR-MVK_KsAg/AAAAAAAAAAI/AAAAAAAALbE/3e3Eq9nrGuw/s64/photo.jpg","userId":"17353049580175403985"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["!unzip kor-eng.zip"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Archive:  kor-eng.zip\n","  inflating: _about.txt              \n","  inflating: kor.txt                 \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TyM-j9T5zNih","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":197},"outputId":"fb88fc4c-4e92-4bd0-9b2e-b7c396a3bec3","executionInfo":{"status":"ok","timestamp":1565159921309,"user_tz":-540,"elapsed":3033,"user":{"displayName":"김화종","photoUrl":"https://lh4.googleusercontent.com/-sR-MVK_KsAg/AAAAAAAAAAI/AAAAAAAALbE/3e3Eq9nrGuw/s64/photo.jpg","userId":"17353049580175403985"}}},"source":["!head -10 kor.txt"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Who?\t누구?\n","Hello!\t안녕!\n","No way!\t절대 아니야.\n","No way!\t그럴리가!\n","Goodbye!\t안녕!\n","I'm sad.\t슬퍼.\n","Me, too.\t나도.\n","Perfect!\t완벽해!\n","Shut up!\t시끄러워!\n","Welcome.\t어서오세요.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mOj5di1x_Ovm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"2a826c2d-0e68-4653-cea0-747745f7885c","executionInfo":{"status":"ok","timestamp":1565160110686,"user_tz":-540,"elapsed":2204,"user":{"displayName":"김화종","photoUrl":"https://lh4.googleusercontent.com/-sR-MVK_KsAg/AAAAAAAAAAI/AAAAAAAALbE/3e3Eq9nrGuw/s64/photo.jpg","userId":"17353049580175403985"}}},"source":["# Deep Learning Quick Reference Chapter 11: Seq2Seq\n","# Mike Bernico <mike.bernico@gmail.com>\n","# This program expects english to french sentance pairs located in chapter_11/data/\n","# Dataset can be found at http://www.manythings.org/anki/fra-eng.zip\n","\n","from keras.models import Model\n","from keras.layers import Input, LSTM, Dense\n","from keras.callbacks import TensorBoard\n","import numpy as np\n","import os\n","from pathlib import Path\n","\n","\n","def load_data(num_samples=50000, start_char='\\t', end_char='\\n', data_path='kor.txt'):\n","    input_texts = []\n","    target_texts = []\n","    input_characters = set()\n","    target_characters = set()\n","    lines = open(Path(data_path), 'r', encoding='utf-8').read().split('\\n')\n","    for line in lines[: min(num_samples, len(lines) - 1)]:\n","        input_text, target_text = line.split('\\t')\n","        target_text = start_char + target_text + end_char\n","        input_texts.append(input_text)\n","        target_texts.append(target_text)\n","        for char in input_text:\n","            if char not in input_characters:\n","                input_characters.add(char)\n","        for char in target_text:\n","            if char not in target_characters:\n","                target_characters.add(char)\n","\n","    input_characters = sorted(list(input_characters))\n","    target_characters = sorted(list(target_characters))\n","    num_encoder_tokens = len(input_characters)\n","    num_decoder_tokens = len(target_characters)\n","    max_encoder_seq_length = max([len(txt) for txt in input_texts])\n","    max_decoder_seq_length = max([len(txt) for txt in target_texts])\n","\n","    print('Number of samples:', len(input_texts))\n","    print('Number of unique input tokens:', num_encoder_tokens)\n","    print('Number of unique output tokens:', num_decoder_tokens)\n","    print('Max sequence length for inputs:', max_encoder_seq_length)\n","    print('Max sequence length for outputs:', max_decoder_seq_length)\n","    return {'input_texts': input_texts, 'target_texts': target_texts,\n","            'input_chars': input_characters, 'target_chars': target_characters,\n","            'num_encoder_tokens': num_encoder_tokens, 'num_decoder_tokens': num_decoder_tokens,\n","            'max_encoder_seq_length': max_encoder_seq_length, 'max_decoder_seq_length': max_decoder_seq_length}\n","\n","\n","def one_hot_vectorize(data):\n","    input_chars = data['input_chars']\n","    target_chars = data['target_chars']\n","    input_texts = data['input_texts']\n","    target_texts = data['target_texts']\n","    max_encoder_seq_length = data['max_encoder_seq_length']\n","    max_decoder_seq_length = data['max_decoder_seq_length']\n","    num_encoder_tokens = data['num_encoder_tokens']\n","    num_decoder_tokens = data['num_decoder_tokens']\n","\n","    input_token_index = dict([(char, i) for i, char in enumerate(input_chars)])\n","    target_token_index = dict([(char, i) for i, char in enumerate(target_chars)])\n","    encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n","    decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n","    decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n","\n","    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n","        for t, char in enumerate(input_text):\n","            encoder_input_data[i, t, input_token_index[char]] = 1.\n","        for t, char in enumerate(target_text):\n","            # decoder_target_data is ahead of decoder_input_data by one timestep\n","            decoder_input_data[i, t, target_token_index[char]] = 1.\n","            if t > 0:\n","                # decoder_target_data will be ahead by one timestep\n","                # and will not include the start character.\n","                decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n","    data['input_token_index'] = input_token_index\n","    data['target_token_index'] = target_token_index\n","    data['encoder_input_data'] = encoder_input_data\n","    data['decoder_input_data'] = decoder_input_data\n","    data['decoder_target_data'] = decoder_target_data\n","    return data\n","\n","\n","def build_models(lstm_units, num_encoder_tokens, num_decoder_tokens):\n","    # train model\n","    encoder_input = Input(shape=(None, num_encoder_tokens), name='encoder_input')\n","    encoder_outputs, state_h, state_c = LSTM(lstm_units, return_state=True, name=\"encoder_lstm\")(encoder_input)\n","    encoder_states = [state_h, state_c]\n","\n","    decoder_input = Input(shape=(None, num_decoder_tokens), name='decoder_input')\n","    decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True,\n","                                 name=\"decoder_lstm\")\n","    decoder_outputs, _, _ = decoder_lstm(decoder_input, initial_state=encoder_states)\n","    decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='softmax_output')\n","    decoder_output = decoder_dense(decoder_outputs)\n","\n","    model = Model([encoder_input, decoder_input], decoder_output)\n","    model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n","\n","    encoder_model = Model(encoder_input, encoder_states)\n","\n","    decoder_state_input_h = Input(shape=(lstm_units,))\n","    decoder_state_input_c = Input(shape=(lstm_units,))\n","    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","    decoder_outputs, state_h, state_c = decoder_lstm(\n","        decoder_input, initial_state=decoder_states_inputs)\n","    decoder_states = [state_h, state_c]\n","    decoder_outputs = decoder_dense(decoder_outputs)\n","    decoder_model = Model(\n","        [decoder_input] + decoder_states_inputs,\n","        [decoder_outputs] + decoder_states)\n","\n","    return model, encoder_model, decoder_model\n","\n","\n","def create_callbacks(name):\n","    tensorboard_callback = TensorBoard(log_dir=os.path.join(os.getcwd(), \"tb_log_char_s2s\", name),\n","                                       write_graph=True,\n","                                       write_grads=False)\n","    return [tensorboard_callback]\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"mlWe6vMk_wXT","colab_type":"code","outputId":"0576d8ac-4389-4a31-914c-9bc9260a6c93","executionInfo":{"status":"ok","timestamp":1565129404807,"user_tz":-540,"elapsed":177026,"user":{"displayName":"김화종","photoUrl":"https://lh4.googleusercontent.com/-sR-MVK_KsAg/AAAAAAAAAAI/AAAAAAAALbE/3e3Eq9nrGuw/s64/photo.jpg","userId":"17353049580175403985"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["from keras.callbacks import EarlyStopping\n","from matplotlib import pyplot\n","\n","data = load_data()\n","data = one_hot_vectorize(data)\n","# callbacks = create_callbacks(\"char_s2s\")\n","model, encoder_model, decoder_model = build_models(256, data['num_encoder_tokens'], data['num_decoder_tokens'])\n","print(model.summary())\n","\n","\n","es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n","\n","\n","history = model.fit(x=[data[\"encoder_input_data\"], data[\"decoder_input_data\"]],\n","          y=data[\"decoder_target_data\"],\n","          batch_size=64,\n","          epochs=400,\n","          validation_split=0.2,\n","          callbacks=[es])\n","#           callbacks=callbacks)\n","\n","pyplot.plot(history.history['loss'], label='train')\n","pyplot.plot(history.history['val_loss'], label='test')\n","pyplot.legend()\n","pyplot.show()\n","\n","model.save('char_s2s_train.h5')\n","encoder_model.save('char_s2s_encoder.h5')\n","decoder_model.save('char_s2s_decoder.h5')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Number of samples: 909\n","Number of unique input tokens: 69\n","Number of unique output tokens: 662\n","Max sequence length for inputs: 124\n","Max sequence length for outputs: 54\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","encoder_input (InputLayer)      (None, None, 69)     0                                            \n","__________________________________________________________________________________________________\n","decoder_input (InputLayer)      (None, None, 662)    0                                            \n","__________________________________________________________________________________________________\n","encoder_lstm (LSTM)             [(None, 256), (None, 333824      encoder_input[0][0]              \n","__________________________________________________________________________________________________\n","decoder_lstm (LSTM)             [(None, None, 256),  941056      decoder_input[0][0]              \n","                                                                 encoder_lstm[0][1]               \n","                                                                 encoder_lstm[0][2]               \n","__________________________________________________________________________________________________\n","softmax_output (Dense)          (None, None, 662)    170134      decoder_lstm[0][0]               \n","==================================================================================================\n","Total params: 1,445,014\n","Trainable params: 1,445,014\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n","Train on 727 samples, validate on 182 samples\n","Epoch 1/400\n","727/727 [==============================] - 6s 9ms/step - loss: 1.3396 - val_loss: 2.1194\n","Epoch 2/400\n","727/727 [==============================] - 3s 4ms/step - loss: 1.1296 - val_loss: 2.1648\n","Epoch 3/400\n","727/727 [==============================] - 3s 4ms/step - loss: 1.1150 - val_loss: 2.1026\n","Epoch 4/400\n","727/727 [==============================] - 3s 4ms/step - loss: 1.1042 - val_loss: 2.1281\n","Epoch 5/400\n","727/727 [==============================] - 3s 4ms/step - loss: 1.0939 - val_loss: 2.0759\n","Epoch 6/400\n","727/727 [==============================] - 3s 4ms/step - loss: 1.0872 - val_loss: 2.0833\n","Epoch 7/400\n","727/727 [==============================] - 3s 4ms/step - loss: 1.0740 - val_loss: 2.1491\n","Epoch 8/400\n","727/727 [==============================] - 3s 4ms/step - loss: 1.0680 - val_loss: 2.2147\n","Epoch 9/400\n","727/727 [==============================] - 3s 4ms/step - loss: 1.0562 - val_loss: 2.1350\n","Epoch 10/400\n","727/727 [==============================] - 3s 4ms/step - loss: 1.0439 - val_loss: 2.0758\n","Epoch 11/400\n","727/727 [==============================] - 3s 4ms/step - loss: 1.0260 - val_loss: 1.9882\n","Epoch 12/400\n","727/727 [==============================] - 3s 4ms/step - loss: 1.0076 - val_loss: 1.9911\n","Epoch 13/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.9999 - val_loss: 1.9822\n","Epoch 14/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.9733 - val_loss: 1.9545\n","Epoch 15/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.9530 - val_loss: 1.9476\n","Epoch 16/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.9350 - val_loss: 1.8914\n","Epoch 17/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.9216 - val_loss: 1.8555\n","Epoch 18/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.9041 - val_loss: 1.8785\n","Epoch 19/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.8893 - val_loss: 1.8082\n","Epoch 20/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.8779 - val_loss: 1.8373\n","Epoch 21/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.8637 - val_loss: 1.8333\n","Epoch 22/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.8501 - val_loss: 1.8307\n","Epoch 23/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.8424 - val_loss: 1.8233\n","Epoch 24/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.8273 - val_loss: 1.7646\n","Epoch 25/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.8171 - val_loss: 1.8085\n","Epoch 26/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.8062 - val_loss: 1.7574\n","Epoch 27/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.7949 - val_loss: 1.7356\n","Epoch 28/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.7843 - val_loss: 1.7735\n","Epoch 29/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.7759 - val_loss: 1.7775\n","Epoch 30/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.7672 - val_loss: 1.7639\n","Epoch 31/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.7536 - val_loss: 1.7821\n","Epoch 32/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.7491 - val_loss: 1.7510\n","Epoch 33/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.7370 - val_loss: 1.7571\n","Epoch 34/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.7271 - val_loss: 1.7313\n","Epoch 35/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.7168 - val_loss: 1.7288\n","Epoch 36/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.7085 - val_loss: 1.7442\n","Epoch 37/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.6994 - val_loss: 1.7564\n","Epoch 38/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.6916 - val_loss: 1.7420\n","Epoch 39/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.6819 - val_loss: 1.7404\n","Epoch 40/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.6701 - val_loss: 1.7314\n","Epoch 41/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.6638 - val_loss: 1.7213\n","Epoch 42/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.6544 - val_loss: 1.7208\n","Epoch 43/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.6457 - val_loss: 1.7451\n","Epoch 44/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.6345 - val_loss: 1.7392\n","Epoch 45/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.6315 - val_loss: 1.7170\n","Epoch 46/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.6193 - val_loss: 1.7303\n","Epoch 47/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.6094 - val_loss: 1.7255\n","Epoch 48/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.6022 - val_loss: 1.7431\n","Epoch 49/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.5960 - val_loss: 1.7001\n","Epoch 50/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.5843 - val_loss: 1.7549\n","Epoch 51/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.5768 - val_loss: 1.7268\n","Epoch 52/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.5706 - val_loss: 1.7279\n","Epoch 53/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.6273 - val_loss: 1.7812\n","Epoch 54/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.6931 - val_loss: 1.7090\n","Epoch 55/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.5711 - val_loss: 1.7269\n","Epoch 56/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.5675 - val_loss: 1.7173\n","Epoch 57/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.5755 - val_loss: 1.7228\n","Epoch 58/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.5340 - val_loss: 1.7289\n","Epoch 59/400\n","727/727 [==============================] - 3s 4ms/step - loss: 0.5345 - val_loss: 1.7045\n","Epoch 00059: early stopping\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/engine/network.py:877: UserWarning: Layer decoder_lstm was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'encoder_lstm_9/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'encoder_lstm_9/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n","  '. They will not be included '\n","/usr/local/lib/python3.6/dist-packages/keras/engine/network.py:877: UserWarning: Layer decoder_lstm was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_11:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'input_12:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n","  '. They will not be included '\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"l6Dt_SlQC3mj","colab_type":"code","outputId":"8cb4197a-33f7-4675-9ba9-a1b34fafb252","executionInfo":{"status":"ok","timestamp":1565128500620,"user_tz":-540,"elapsed":1105278,"user":{"displayName":"김화종","photoUrl":"https://lh4.googleusercontent.com/-sR-MVK_KsAg/AAAAAAAAAAI/AAAAAAAALbE/3e3Eq9nrGuw/s64/photo.jpg","userId":"17353049580175403985"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["!ls"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_about.txt\t     char_s2s_train.h5\tkor-eng.zip  tb_log_char_s2s\n","char_s2s_decoder.h5  fra-eng.zip\tkor.txt\n","char_s2s_encoder.h5  fra.txt\t\tsample_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"S4mL8K1wC4ab","colab_type":"code","colab":{}},"source":["from keras.models import Model, load_model\n","# from train_char_seq2seq import load_data, one_hot_vectorize\n","import numpy as np\n","\n","\n","def load_models():\n","    model = load_model('char_s2s_train.h5')\n","    encoder_model = load_model('char_s2s_encoder.h5')\n","    decoder_model = load_model('char_s2s_decoder.h5')\n","    return [model, encoder_model, decoder_model]\n","\n","\n","def decode_sequence(input_seq, data, encoder_model, decoder_model):\n","    # Encode the input as state vectors.\n","    states_value = encoder_model.predict(input_seq)\n","\n","    # Generate empty target sequence of length 1.\n","    target_seq = np.zeros((1, 1, data['num_decoder_tokens']))\n","    # Populate the first character of target sequence with the start character.\n","    target_seq[0, 0, data['target_token_index']['\\t']] = 1.\n","\n","    # Sampling loop for a batch of sequences\n","    # (to simplify, here we assume a batch of size 1).\n","    stop_condition = False\n","    decoded_sentence = ''\n","    while not stop_condition:\n","        output_tokens, h, c = decoder_model.predict(\n","            [target_seq] + states_value)\n","\n","        # Sample a token\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","        sampled_char = data[\"reverse_target_char_index\"][sampled_token_index]\n","        decoded_sentence += sampled_char\n","\n","        # Exit condition: either hit max length\n","        # or find stop character.\n","        if (sampled_char == '\\n' or\n","           len(decoded_sentence) > data['max_decoder_seq_length']):\n","            stop_condition = True\n","\n","        # Update the target sequence (of length 1).\n","        target_seq = np.zeros((1, 1, data['num_decoder_tokens']))\n","        target_seq[0, 0, sampled_token_index] = 1.\n","\n","        # Update states\n","        states_value = [h, c]\n","\n","    return decoded_sentence\n","\n","\n","def create_reverse_indicies(data):\n","    data['reverse_input_char_index'] = dict(\n","        (i, char) for char, i in data[\"input_token_index\"].items())\n","    data['reverse_target_char_index'] = dict(\n","        (i, char) for char, i in data[\"target_token_index\"].items())\n","    return data\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"71OK-denGfP6","colab_type":"code","outputId":"04aa67c6-2209-4a61-a74b-5f1869c5b52a","executionInfo":{"status":"ok","timestamp":1565128514085,"user_tz":-540,"elapsed":1118725,"user":{"displayName":"김화종","photoUrl":"https://lh4.googleusercontent.com/-sR-MVK_KsAg/AAAAAAAAAAI/AAAAAAAALbE/3e3Eq9nrGuw/s64/photo.jpg","userId":"17353049580175403985"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["data = load_data()\n","data = one_hot_vectorize(data)\n","data = create_reverse_indicies(data)\n","model, encoder_model, decoder_model = load_models()\n","\n","for seq_index in range(100):\n","    # Take one sequence (part of the training set)\n","    # for trying out decoding.\n","    input_seq = data[\"encoder_input_data\"][seq_index: seq_index + 1]\n","    decoded_sentence = decode_sequence(input_seq, data, encoder_model, decoder_model)\n","    print('-')\n","    print('Input sentence:', data['input_texts'][seq_index])\n","    print('Correct Translation:', data['target_texts'][seq_index].strip(\"\\t\\n\"))\n","    print('Decoded sentence:', decoded_sentence)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Number of samples: 909\n","Number of unique input tokens: 69\n","Number of unique output tokens: 662\n","Max sequence length for inputs: 124\n","Max sequence length for outputs: 54\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n","  warnings.warn('No training configuration found in save file: '\n"],"name":"stderr"},{"output_type":"stream","text":["-\n","Input sentence: Who?\n","Correct Translation: 누구?\n","Decoded sentence: 좋아하는 가수는 누구예요?\n","\n","-\n","Input sentence: Hello!\n","Correct Translation: 안녕!\n","Decoded sentence: 우리는 톰과 즐거운 시간을 가졌다.\n","\n","-\n","Input sentence: No way!\n","Correct Translation: 절대 아니야.\n","Decoded sentence: 나도 너랑 같은 처지야.\n","\n","-\n","Input sentence: No way!\n","Correct Translation: 그럴리가!\n","Decoded sentence: 나도 너랑 같은 처지야.\n","\n","-\n","Input sentence: Goodbye!\n","Correct Translation: 안녕!\n","Decoded sentence: 우리는 그 사건을 검토할 필요가 있다.\n","\n","-\n","Input sentence: I'm sad.\n","Correct Translation: 슬퍼.\n","Decoded sentence: 나도 너랑 같은 처지야.\n","\n","-\n","Input sentence: Me, too.\n","Correct Translation: 나도.\n","Decoded sentence: 그건 무슨 언어였지?\n","\n","-\n","Input sentence: Perfect!\n","Correct Translation: 완벽해!\n","Decoded sentence: 이게 대체 뭐예요?\n","\n","-\n","Input sentence: Shut up!\n","Correct Translation: 시끄러워!\n","Decoded sentence: 우리는 그 사건을 검토할 필요가 있다.\n","\n","-\n","Input sentence: Welcome.\n","Correct Translation: 어서오세요.\n","Decoded sentence: 거짓말 하지 마세요.\n","\n","-\n","Input sentence: Welcome.\n","Correct Translation: 환영합니다.\n","Decoded sentence: 거짓말 하지 마세요.\n","\n","-\n","Input sentence: Cheer up!\n","Correct Translation: 힘내!\n","Decoded sentence: 이 번호로 전화해.\n","\n","-\n","Input sentence: Get lost.\n","Correct Translation: 꺼져.\n","Decoded sentence: 우리는 톰과 즐거운 시간을 가졌다.\n","\n","-\n","Input sentence: I'm ugly.\n","Correct Translation: 나는 못 생겼다.\n","Decoded sentence: 나도 같은 처지다.\n","\n","-\n","Input sentence: It hurts.\n","Correct Translation: 아파.\n","Decoded sentence: 나도 행복하고 싶다.\n","\n","-\n","Input sentence: Let's go!\n","Correct Translation: 가자!\n","Decoded sentence: 이제 충분히 했습니다.\n","\n","-\n","Input sentence: Don't lie.\n","Correct Translation: 거짓말 하지 마.\n","Decoded sentence: 거짓말 하지 마세요.\n","\n","-\n","Input sentence: Don't lie.\n","Correct Translation: 거짓말 하지 마세요.\n","Decoded sentence: 거짓말 하지 마세요.\n","\n","-\n","Input sentence: I'm sorry.\n","Correct Translation: 미안해.\n","Decoded sentence: 나도 같은 처지다.\n","\n","-\n","Input sentence: I'm sorry.\n","Correct Translation: 미안해요.\n","Decoded sentence: 나도 같은 처지다.\n","\n","-\n","Input sentence: I'm sorry.\n","Correct Translation: 죄송합니다.\n","Decoded sentence: 나도 같은 처지다.\n","\n","-\n","Input sentence: I'm sorry.\n","Correct Translation: 유감입니다.\n","Decoded sentence: 나도 같은 처지다.\n","\n","-\n","Input sentence: Of course.\n","Correct Translation: 물론이죠.\n","Decoded sentence: 이제 충분히 했습니다.\n","\n","-\n","Input sentence: Seriously?\n","Correct Translation: 진심이야?\n","Decoded sentence: 우리는 톰과 즐거운 시간을 가졌다.\n","\n","-\n","Input sentence: Take care.\n","Correct Translation: 주의하세요.\n","Decoded sentence: 그녀는 우울하다.\n","\n","-\n","Input sentence: Be careful.\n","Correct Translation: 조심해!\n","Decoded sentence: 거짓말 하지 마세요.\n","\n","-\n","Input sentence: He is nice.\n","Correct Translation: 걔 괜찮아.\n","Decoded sentence: 그는 그 문제로 난처해졌다.\n","\n","-\n","Input sentence: Hold still.\n","Correct Translation: 가만히 있으세요.\n","Decoded sentence: 어찌나 사랑스러운지!\n","\n","-\n","Input sentence: Hold still.\n","Correct Translation: 가만히 있어.\n","Decoded sentence: 어찌나 사랑스러운지!\n","\n","-\n","Input sentence: How lovely!\n","Correct Translation: 어찌나 사랑스러운지!\n","Decoded sentence: 가만히 있어.\n","\n","-\n","Input sentence: I felt bad.\n","Correct Translation: 난 기분이 나빴다.\n","Decoded sentence: 나도 몰라.\n","\n","-\n","Input sentence: Is that OK?\n","Correct Translation: 괜찮은 거예요?\n","Decoded sentence: 이거 내꺼니?\n","\n","-\n","Input sentence: Love hurts.\n","Correct Translation: 사랑은 아프다.\n","Decoded sentence: 나도 너랑 같은 처지야.\n","\n","-\n","Input sentence: Boys do cry.\n","Correct Translation: 남자애도 운다.\n","Decoded sentence: 우리 애들은 학교에 있어요.\n","\n","-\n","Input sentence: I don't lie.\n","Correct Translation: 나는 거짓말 하지 않습니다.\n","Decoded sentence: 나도 몰라.\n","\n","-\n","Input sentence: I don't lie.\n","Correct Translation: 난 거짓말 안해.\n","Decoded sentence: 나도 몰라.\n","\n","-\n","Input sentence: I don't lie.\n","Correct Translation: 나는 거짓말 하지 않아.\n","Decoded sentence: 나도 몰라.\n","\n","-\n","Input sentence: I'm nervous.\n","Correct Translation: 긴장돼요.\n","Decoded sentence: 나도 같은 처지다.\n","\n","-\n","Input sentence: I'm nervous.\n","Correct Translation: 떨려요.\n","Decoded sentence: 나도 같은 처지다.\n","\n","-\n","Input sentence: I'm shocked.\n","Correct Translation: 충격이야.\n","Decoded sentence: 나도 같은 처지다.\n","\n","-\n","Input sentence: It's a pity.\n","Correct Translation: 안타까워요.\n","Decoded sentence: 나도 같은 처지다.\n","\n","-\n","Input sentence: What's that?\n","Correct Translation: 저건 뭐야?\n","Decoded sentence: 그건 무슨 언어였지?\n","\n","-\n","Input sentence: You're mine.\n","Correct Translation: 넌 내 거야.\n","Decoded sentence: 이야기를 마저 들어보자.\n","\n","-\n","Input sentence: You're mine.\n","Correct Translation: 당신은 나의 것입니다.\n","Decoded sentence: 이야기를 마저 들어보자.\n","\n","-\n","Input sentence: Blood is red.\n","Correct Translation: 피는 붉다.\n","Decoded sentence: 좋아하는 가수는 누구예요?\n","\n","-\n","Input sentence: Can I go now?\n","Correct Translation: 이제 가도 되나요?\n","Decoded sentence: 이 번호로 전화해.\n","\n","-\n","Input sentence: Come quickly!\n","Correct Translation: 빨리 와!\n","Decoded sentence: 이 번호로 전화해.\n","\n","-\n","Input sentence: Come quickly!\n","Correct Translation: 빨리 오세요!\n","Decoded sentence: 이 번호로 전화해.\n","\n","-\n","Input sentence: Don't eat it.\n","Correct Translation: 먹지 마.\n","Decoded sentence: 거짓말 하지 마세요.\n","\n","-\n","Input sentence: I don't know.\n","Correct Translation: 나는 몰라요.\n","Decoded sentence: 나는 그것을 버리고 싶지 않다.\n","\n","-\n","Input sentence: I hate liars.\n","Correct Translation: 난 거짓말쟁이가 싫어.\n","Decoded sentence: 나는 내년에 중국어를 배우고 싶다.\n","\n","-\n","Input sentence: I need money.\n","Correct Translation: 돈이 필요해요.\n","Decoded sentence: 나는 내년에 중국어를 배우고 싶다.\n","\n","-\n","Input sentence: Is that okay?\n","Correct Translation: 괜찮은 거예요?\n","Decoded sentence: 이거 내꺼니?\n","\n","-\n","Input sentence: Is this mine?\n","Correct Translation: 이거 내꺼니?\n","Decoded sentence: 이거 내꺼니?\n","\n","-\n","Input sentence: Is this wine?\n","Correct Translation: 이게 와인이야?\n","Decoded sentence: 이거 내꺼니?\n","\n","-\n","Input sentence: It's suicide.\n","Correct Translation: 자살입니다.\n","Decoded sentence: 나도 같은 처지다.\n","\n","-\n","Input sentence: Just keep it.\n","Correct Translation: 그냥 그거 가져.\n","Decoded sentence: 그건 무슨 언어였지?\n","\n","-\n","Input sentence: We don't lie.\n","Correct Translation: 우리는 거짓말을 하지 않아요.\n","Decoded sentence: 우리는 톰과 즐거운 시간을 가졌다.\n","\n","-\n","Input sentence: We don't lie.\n","Correct Translation: 우린 거짓말 안해.\n","Decoded sentence: 우리는 톰과 즐거운 시간을 가졌다.\n","\n","-\n","Input sentence: We're inside.\n","Correct Translation: 우리 안에 들어와 있어.\n","Decoded sentence: 우리는 톰과 즐거운 시간을 가졌다.\n","\n","-\n","Input sentence: We're inside.\n","Correct Translation: 우린 안에 있어요.\n","Decoded sentence: 우리는 톰과 즐거운 시간을 가졌다.\n","\n","-\n","Input sentence: What is that?\n","Correct Translation: 저것은 무엇입니까?\n","Decoded sentence: 그가 여기에 오면 저한테 말해주세요.\n","\n","-\n","Input sentence: Can I ask why?\n","Correct Translation: 이유를 물어봐도 돼?\n","Decoded sentence: 이 번호로 전화해.\n","\n","-\n","Input sentence: Grab the rope.\n","Correct Translation: 로프를 잡으세요.\n","Decoded sentence: 이게 대체 뭐예요?\n","\n","-\n","Input sentence: I am homesick.\n","Correct Translation: 나 향수병 걸렸어.\n","Decoded sentence: 나도 행복하고 싶다.\n","\n","-\n","Input sentence: I can't sleep.\n","Correct Translation: 잠이 와.\n","Decoded sentence: 나도 행복하고 싶다.\n","\n","-\n","Input sentence: I feel guilty.\n","Correct Translation: 죄책감이 들어.\n","Decoded sentence: 나도 몰라.\n","\n","-\n","Input sentence: I hate myself.\n","Correct Translation: 나는 내 자신이 싫어.\n","Decoded sentence: 나는 내년에 중국어를 배우고 싶다.\n","\n","-\n","Input sentence: I smell blood.\n","Correct Translation: 피 냄새가 납니다.\n","Decoded sentence: 나도 행복하고 싶다.\n","\n","-\n","Input sentence: I use Firefox.\n","Correct Translation: 나는 파이어폭스를 사용해.\n","Decoded sentence: 나는 내년에 중국어를 배우고 싶다.\n","\n","-\n","Input sentence: I want to die.\n","Correct Translation: 죽고 싶어요.\n","Decoded sentence: 나에게 사전이 있다.\n","\n","-\n","Input sentence: I want to die.\n","Correct Translation: 죽고 싶어.\n","Decoded sentence: 나에게 사전이 있다.\n","\n","-\n","Input sentence: I'll kill him.\n","Correct Translation: 나는 그를 죽일 것이다.\n","Decoded sentence: 나도 같은 처지다.\n","\n","-\n","Input sentence: I'm depressed.\n","Correct Translation: 우울해.\n","Decoded sentence: 나도 같은 처지다.\n","\n","-\n","Input sentence: Is that blood?\n","Correct Translation: 그거 피야?\n","Decoded sentence: 이거 내꺼니?\n","\n","-\n","Input sentence: My head hurts.\n","Correct Translation: 머리가 아파요.\n","Decoded sentence: 내 친구는 슈퍼마켓에서 일해.\n","\n","-\n","Input sentence: Tom is honest.\n","Correct Translation: 톰은 정직하다.\n","Decoded sentence: 톰은 그의 차를 메리에게 팔았다.\n","\n","-\n","Input sentence: We want peace.\n","Correct Translation: 우리는 평화를 원합니다.\n","Decoded sentence: 우리는 톰과 즐거운 시간을 가졌다.\n","\n","-\n","Input sentence: Autumn is here.\n","Correct Translation: 가을이 되었습니다.\n","Decoded sentence: 내 친구는 슈퍼마켓에서 일해.\n","\n","-\n","Input sentence: Autumn is here.\n","Correct Translation: 가을이 왔어요.\n","Decoded sentence: 내 친구는 슈퍼마켓에서 일해.\n","\n","-\n","Input sentence: Can I help you?\n","Correct Translation: 제가 좀 도와 드릴까요?\n","Decoded sentence: 이 번호로 전화해.\n","\n","-\n","Input sentence: Do you hear me?\n","Correct Translation: 제 말이 들리세요?\n","Decoded sentence: 좋아하는 가수는 누구예요?\n","\n","-\n","Input sentence: He was hard up.\n","Correct Translation: 그는 돈에 쪼들리고 있었다.\n","Decoded sentence: 그는 그 문제로 난처해졌다.\n","\n","-\n","Input sentence: I don't buy it.\n","Correct Translation: 못 믿어.\n","Decoded sentence: 나는 그것을 버리고 싶지 않다.\n","\n","-\n","Input sentence: I like reading.\n","Correct Translation: 독서를 좋아합니다.\n","Decoded sentence: 나도 행복하고 싶다.\n","\n","-\n","Input sentence: I love lasagna.\n","Correct Translation: 저는 라자냐를 좋아해요.\n","Decoded sentence: 나는 내년에 중국어를 배우고 싶다.\n","\n","-\n","Input sentence: I love my home.\n","Correct Translation: 난 내 집이 좋아.\n","Decoded sentence: 나는 내년에 중국어를 배우고 싶다.\n","\n","-\n","Input sentence: I study Korean.\n","Correct Translation: 한국말을 공부합니다.\n","Decoded sentence: 나는 내년에 중국어를 배우고 싶다.\n","\n","-\n","Input sentence: I'm very sorry.\n","Correct Translation: 정말 미안해.\n","Decoded sentence: 나도 행복하고 싶다.\n","\n","-\n","Input sentence: I'm very sorry.\n","Correct Translation: 정말 죄송합니다.\n","Decoded sentence: 나도 행복하고 싶다.\n","\n","-\n","Input sentence: Keep Tom there.\n","Correct Translation: 톰은 여기에 두세요.\n","Decoded sentence: 내가 도와줄게.\n","\n","-\n","Input sentence: Only God knows.\n","Correct Translation: 신만이 아실 것입니다.\n","Decoded sentence: 이 사전은 비싸다.\n","\n","-\n","Input sentence: Read this book.\n","Correct Translation: 이 책 읽어.\n","Decoded sentence: 이 흙에서는 아무것도 자라지 않는 것으로 보인다.\n","\n","-\n","Input sentence: Read this book.\n","Correct Translation: 이 책을 읽으세요.\n","Decoded sentence: 이 흙에서는 아무것도 자라지 않는 것으로 보인다.\n","\n","-\n","Input sentence: Sorry I'm late.\n","Correct Translation: 늦어서 미안해.\n","Decoded sentence: 이 번호로 전화해.\n","\n","-\n","Input sentence: That's suicide.\n","Correct Translation: 그것은 자살입니다.\n","Decoded sentence: 그 약은 효과가 있었다.\n","\n","-\n","Input sentence: Boil some water.\n","Correct Translation: 물 좀 끓여.\n","Decoded sentence: 우리는 그 사건을 검토할 필요가 있다.\n","\n","-\n","Input sentence: Can you help me?\n","Correct Translation: 저를 좀 도와 주실래요?\n","Decoded sentence: 이 번호로 전화해.\n","\n","-\n","Input sentence: Congratulations!\n","Correct Translation: 축하해!\n","Decoded sentence: 그게 와인을 고 해해?\n","\n","-\n","Input sentence: Do you like rap?\n","Correct Translation: 랩 좋아해요?\n","Decoded sentence: 좋아하는 가수는 누구예요?\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WmG1kNGoIFh8","colab_type":"code","outputId":"88d07d63-b68d-4f86-f3e3-5248a9c92e78","executionInfo":{"status":"ok","timestamp":1565129458258,"user_tz":-540,"elapsed":20923,"user":{"displayName":"김화종","photoUrl":"https://lh4.googleusercontent.com/-sR-MVK_KsAg/AAAAAAAAAAI/AAAAAAAALbE/3e3Eq9nrGuw/s64/photo.jpg","userId":"17353049580175403985"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["data = load_data()\n","data = one_hot_vectorize(data)\n","data = create_reverse_indicies(data)\n","model, encoder_model, decoder_model = load_models()\n","\n","for seq_index in range(100):\n","    # Take one sequence (part of the training set)\n","    # for trying out decoding.\n","    input_seq = data[\"encoder_input_data\"][seq_index: seq_index + 1]\n","    decoded_sentence = decode_sequence(input_seq, data, encoder_model, decoder_model)\n","    print('-')\n","    print('Input sentence:', data['input_texts'][seq_index])\n","    print('Correct Translation:', data['target_texts'][seq_index].strip(\"\\t\\n\"))\n","    print('Decoded sentence:', decoded_sentence)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Number of samples: 909\n","Number of unique input tokens: 69\n","Number of unique output tokens: 662\n","Max sequence length for inputs: 124\n","Max sequence length for outputs: 54\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n","  warnings.warn('No training configuration found in save file: '\n"],"name":"stderr"},{"output_type":"stream","text":["-\n","Input sentence: Who?\n","Correct Translation: 누구?\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: Hello!\n","Correct Translation: 안녕!\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: No way!\n","Correct Translation: 절대 아니야.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: No way!\n","Correct Translation: 그럴리가!\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: Goodbye!\n","Correct Translation: 안녕!\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: I'm sad.\n","Correct Translation: 슬퍼.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: Me, too.\n","Correct Translation: 나도.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: Perfect!\n","Correct Translation: 완벽해!\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: Shut up!\n","Correct Translation: 시끄러워!\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: Welcome.\n","Correct Translation: 어서오세요.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: Welcome.\n","Correct Translation: 환영합니다.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: Cheer up!\n","Correct Translation: 힘내!\n","Decoded sentence: 톰은 그를 사랑한다.\n","\n","-\n","Input sentence: Get lost.\n","Correct Translation: 꺼져.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: I'm ugly.\n","Correct Translation: 나는 못 생겼다.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: It hurts.\n","Correct Translation: 아파.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: Let's go!\n","Correct Translation: 가자!\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: Don't lie.\n","Correct Translation: 거짓말 하지 마.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: Don't lie.\n","Correct Translation: 거짓말 하지 마세요.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: I'm sorry.\n","Correct Translation: 미안해.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: I'm sorry.\n","Correct Translation: 미안해요.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: I'm sorry.\n","Correct Translation: 죄송합니다.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: I'm sorry.\n","Correct Translation: 유감입니다.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: Of course.\n","Correct Translation: 물론이죠.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: Seriously?\n","Correct Translation: 진심이야?\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: Take care.\n","Correct Translation: 주의하세요.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: Be careful.\n","Correct Translation: 조심해!\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: He is nice.\n","Correct Translation: 걔 괜찮아.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: Hold still.\n","Correct Translation: 가만히 있으세요.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: Hold still.\n","Correct Translation: 가만히 있어.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: How lovely!\n","Correct Translation: 어찌나 사랑스러운지!\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: I felt bad.\n","Correct Translation: 난 기분이 나빴다.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: Is that OK?\n","Correct Translation: 괜찮은 거예요?\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: Love hurts.\n","Correct Translation: 사랑은 아프다.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: Boys do cry.\n","Correct Translation: 남자애도 운다.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: I don't lie.\n","Correct Translation: 나는 거짓말 하지 않습니다.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: I don't lie.\n","Correct Translation: 난 거짓말 안해.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: I don't lie.\n","Correct Translation: 나는 거짓말 하지 않아.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: I'm nervous.\n","Correct Translation: 긴장돼요.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: I'm nervous.\n","Correct Translation: 떨려요.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: I'm shocked.\n","Correct Translation: 충격이야.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: It's a pity.\n","Correct Translation: 안타까워요.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: What's that?\n","Correct Translation: 저건 뭐야?\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: You're mine.\n","Correct Translation: 넌 내 거야.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: You're mine.\n","Correct Translation: 당신은 나의 것입니다.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: Blood is red.\n","Correct Translation: 피는 붉다.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: Can I go now?\n","Correct Translation: 이제 가도 되나요?\n","Decoded sentence: 톰은 그를 사랑한다.\n","\n","-\n","Input sentence: Come quickly!\n","Correct Translation: 빨리 와!\n","Decoded sentence: 그는 그 사전을 다고 했다.\n","\n","-\n","Input sentence: Come quickly!\n","Correct Translation: 빨리 오세요!\n","Decoded sentence: 그는 그 사전을 다고 했다.\n","\n","-\n","Input sentence: Don't eat it.\n","Correct Translation: 먹지 마.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: I don't know.\n","Correct Translation: 나는 몰라요.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: I hate liars.\n","Correct Translation: 난 거짓말쟁이가 싫어.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: I need money.\n","Correct Translation: 돈이 필요해요.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: Is that okay?\n","Correct Translation: 괜찮은 거예요?\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: Is this mine?\n","Correct Translation: 이거 내꺼니?\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: Is this wine?\n","Correct Translation: 이게 와인이야?\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: It's suicide.\n","Correct Translation: 자살입니다.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: Just keep it.\n","Correct Translation: 그냥 그거 가져.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: We don't lie.\n","Correct Translation: 우리는 거짓말을 하지 않아요.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: We don't lie.\n","Correct Translation: 우린 거짓말 안해.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: We're inside.\n","Correct Translation: 우리 안에 들어와 있어.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: We're inside.\n","Correct Translation: 우린 안에 있어요.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: What is that?\n","Correct Translation: 저것은 무엇입니까?\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: Can I ask why?\n","Correct Translation: 이유를 물어봐도 돼?\n","Decoded sentence: 톰은 그를 사랑한다.\n","\n","-\n","Input sentence: Grab the rope.\n","Correct Translation: 로프를 잡으세요.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: I am homesick.\n","Correct Translation: 나 향수병 걸렸어.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: I can't sleep.\n","Correct Translation: 잠이 와.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: I feel guilty.\n","Correct Translation: 죄책감이 들어.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: I hate myself.\n","Correct Translation: 나는 내 자신이 싫어.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: I smell blood.\n","Correct Translation: 피 냄새가 납니다.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: I use Firefox.\n","Correct Translation: 나는 파이어폭스를 사용해.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: I want to die.\n","Correct Translation: 죽고 싶어요.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: I want to die.\n","Correct Translation: 죽고 싶어.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: I'll kill him.\n","Correct Translation: 나는 그를 죽일 것이다.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: I'm depressed.\n","Correct Translation: 우울해.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: Is that blood?\n","Correct Translation: 그거 피야?\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: My head hurts.\n","Correct Translation: 머리가 아파요.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: Tom is honest.\n","Correct Translation: 톰은 정직하다.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: We want peace.\n","Correct Translation: 우리는 평화를 원합니다.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: Autumn is here.\n","Correct Translation: 가을이 되었습니다.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: Autumn is here.\n","Correct Translation: 가을이 왔어요.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: Can I help you?\n","Correct Translation: 제가 좀 도와 드릴까요?\n","Decoded sentence: 톰은 그를 사랑한다.\n","\n","-\n","Input sentence: Do you hear me?\n","Correct Translation: 제 말이 들리세요?\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: He was hard up.\n","Correct Translation: 그는 돈에 쪼들리고 있었다.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: I don't buy it.\n","Correct Translation: 못 믿어.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: I like reading.\n","Correct Translation: 독서를 좋아합니다.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: I love lasagna.\n","Correct Translation: 저는 라자냐를 좋아해요.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: I love my home.\n","Correct Translation: 난 내 집이 좋아.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: I study Korean.\n","Correct Translation: 한국말을 공부합니다.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: I'm very sorry.\n","Correct Translation: 정말 미안해.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: I'm very sorry.\n","Correct Translation: 정말 죄송합니다.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: Keep Tom there.\n","Correct Translation: 톰은 여기에 두세요.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: Only God knows.\n","Correct Translation: 신만이 아실 것입니다.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: Read this book.\n","Correct Translation: 이 책 읽어.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: Read this book.\n","Correct Translation: 이 책을 읽으세요.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: Sorry I'm late.\n","Correct Translation: 늦어서 미안해.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: That's suicide.\n","Correct Translation: 그것은 자살입니다.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: Boil some water.\n","Correct Translation: 물 좀 끓여.\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n","-\n","Input sentence: Can you help me?\n","Correct Translation: 저를 좀 도와 주실래요?\n","Decoded sentence: 톰은 그를 사랑한다.\n","\n","-\n","Input sentence: Congratulations!\n","Correct Translation: 축하해!\n","Decoded sentence: 그는 그 사전을 다고 했다.\n","\n","-\n","Input sentence: Do you like rap?\n","Correct Translation: 랩 좋아해요?\n","Decoded sentence: 톰은 그를 무엇을 했어.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"I2VtmKOl-_AT","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}