{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hh_케라스임베딩_문서분류.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iqit_jgnogSx",
        "colab_type": "text"
      },
      "source": [
        "'''This is a fork/refactor of https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py\n",
        "This program trains a newsgroup classifier with a pretrained keras embedding layer. It contrasts\n",
        "'newsgroup_classifier__word_embeddings.py' which does not use pretrained GloVe vectors.\n",
        "GloVe embedding data can be found at:\n",
        "http://nlp.stanford.edu/data/glove.6B.zip\n",
        "(source page: http://nlp.stanford.edu/projects/glove/)\n",
        "20 Newsgroup data can be found at:\n",
        "http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.html\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2w-FQPaboq_p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "9a7c0d6a-6c51-4740-e744-3d43f0fb0ce8"
      },
      "source": [
        "!wget http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz \\\n",
        "  -O news20.tar.gz"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-07-26 23:34:46--  http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz\n",
            "Resolving www.cs.cmu.edu (www.cs.cmu.edu)... 128.2.42.95\n",
            "Connecting to www.cs.cmu.edu (www.cs.cmu.edu)|128.2.42.95|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17329808 (17M) [application/x-tar]\n",
            "Saving to: ‘news20.tar.gz’\n",
            "\n",
            "news20.tar.gz       100%[===================>]  16.53M  1.62MB/s    in 9.9s    \n",
            "\n",
            "2019-07-26 23:34:56 (1.67 MB/s) - ‘news20.tar.gz’ saved [17329808/17329808]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFB27jfWqDu0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "outputId": "08b64225-e135-4f51-f101-daac679c7c52"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip \\\n",
        "  -O glove.6B.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-07-26 23:35:01--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2019-07-26 23:35:01--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2019-07-26 23:35:02--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  63.9MB/s    in 13s     \n",
            "\n",
            "2019-07-26 23:35:15 (61.3 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtX_eHlEqVjC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "outputId": "a73b525d-9cf5-4213-be8e-b1f4cad5f25d"
      },
      "source": [
        "!unzip glove.6B.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVrEiOOzpE9U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar -zxf news20.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GC65Obv4pvxW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "91ea814b-52b7-4081-978c-df3d7ea38343"
      },
      "source": [
        "!ls 20_newsgroup/"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "alt.atheism\t\t  rec.autos\t      sci.space\n",
            "comp.graphics\t\t  rec.motorcycles     soc.religion.christian\n",
            "comp.os.ms-windows.misc   rec.sport.baseball  talk.politics.guns\n",
            "comp.sys.ibm.pc.hardware  rec.sport.hockey    talk.politics.mideast\n",
            "comp.sys.mac.hardware\t  sci.crypt\t      talk.politics.misc\n",
            "comp.windows.x\t\t  sci.electronics     talk.religion.misc\n",
            "misc.forsale\t\t  sci.med\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwxPkIJRn71d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "e758cd54-edf5-441c-ba8c-ab3013bd0c13"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
        "from keras.models import Model\n",
        "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def load_word_vectors(glove_dir):\n",
        "    print('Indexing word vectors.')\n",
        "\n",
        "    embeddings_index = {}\n",
        "    f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'), encoding='utf8')\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "    f.close()\n",
        "\n",
        "    print('Found %s word vectors.' % len(embeddings_index))\n",
        "    return embeddings_index\n",
        "\n",
        "\n",
        "def load_data(text_data_dir, vocab_size, sequence_length, validation_split=0.2):\n",
        "    data = dict()\n",
        "    data[\"vocab_size\"] = vocab_size\n",
        "    data[\"sequence_length\"] = sequence_length\n",
        "\n",
        "    # second, prepare text samples and their labels\n",
        "    print('Processing text dataset')\n",
        "\n",
        "    texts = []  # list of text samples\n",
        "    labels_index = {}  # dictionary mapping label name to numeric id\n",
        "    labels = []  # list of label ids\n",
        "    for name in sorted(os.listdir(text_data_dir)):\n",
        "        path = os.path.join(text_data_dir, name)\n",
        "        if os.path.isdir(path):\n",
        "            label_id = len(labels_index)\n",
        "            labels_index[name] = label_id\n",
        "            for fname in sorted(os.listdir(path)):\n",
        "                if fname.isdigit():\n",
        "                    fpath = os.path.join(path, fname)\n",
        "                    if sys.version_info < (3,):\n",
        "                        f = open(fpath)\n",
        "                    else:\n",
        "                        f = open(fpath, encoding='latin-1')\n",
        "                    t = f.read()\n",
        "                    i = t.find('\\n\\n')  # skip header\n",
        "                    if 0 < i:\n",
        "                        t = t[i:]\n",
        "                    texts.append(t)\n",
        "                    f.close()\n",
        "                    labels.append(label_id)\n",
        "    print('Found %s texts.' % len(texts))\n",
        "    data[\"texts\"] = texts\n",
        "    data[\"labels\"] = labels\n",
        "    return data\n",
        "\n",
        "\n",
        "def tokenize_text(data):\n",
        "    tokenizer = Tokenizer(num_words=data[\"vocab_size\"])\n",
        "    tokenizer.fit_on_texts(data[\"texts\"])\n",
        "    data[\"tokenizer\"] = tokenizer\n",
        "    sequences = tokenizer.texts_to_sequences(data[\"texts\"])\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "    data[\"X\"] = pad_sequences(sequences, maxlen=data[\"sequence_length\"])\n",
        "    data[\"y\"] = to_categorical(np.asarray(data[\"labels\"]))\n",
        "    print('Shape of data tensor:', data[\"X\"].shape)\n",
        "    print('Shape of label tensor:', data[\"y\"].shape)\n",
        "\n",
        "    # texts and labels aren't needed anymore\n",
        "    data.pop(\"texts\", None)\n",
        "    data.pop(\"labels\", None)\n",
        "    return data\n",
        "\n",
        "\n",
        "def train_val_test_split(data):\n",
        "\n",
        "    data[\"X_train\"], X_test_val, data[\"y_train\"],  y_test_val = train_test_split(data[\"X\"], data[\"y\"],\n",
        "                                                                                 test_size=0.2,\n",
        "                                                                                 random_state=42)\n",
        "    data[\"X_val\"], data[\"X_test\"], data[\"y_val\"], data[\"y_test\"] = train_test_split(X_test_val, y_test_val,\n",
        "                                                                                    test_size=0.25,\n",
        "                                                                                    random_state=42)\n",
        "    return data\n",
        "\n",
        "\n",
        "def embedding_index_to_matrix(embeddings_index, vocab_size, embedding_dim, word_index):\n",
        "    print('Preparing embedding matrix.')\n",
        "\n",
        "    # prepare embedding matrix\n",
        "    num_words = min(vocab_size, len(word_index))\n",
        "    embedding_matrix = np.zeros((num_words, embedding_dim))\n",
        "    for word, i in word_index.items():\n",
        "        if i >= vocab_size:\n",
        "            continue\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    return embedding_matrix\n",
        "\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, sequence_length, embedding_matrix):\n",
        "\n",
        "    sequence_input = Input(shape=(sequence_length,), dtype='int32')\n",
        "    embedding_layer = Embedding(input_dim=vocab_size,\n",
        "                                output_dim=embedding_dim,\n",
        "                                weights=[embedding_matrix],\n",
        "                                input_length=sequence_length,\n",
        "                                trainable=False,\n",
        "                                name=\"embedding\")(sequence_input)\n",
        "    x = Conv1D(128, 5, activation='relu')(embedding_layer)\n",
        "    x = MaxPooling1D(5)(x)\n",
        "    x = Conv1D(128, 5, activation='relu')(x)\n",
        "    x = MaxPooling1D(5)(x)\n",
        "    x = Conv1D(128, 5, activation='relu')(x)\n",
        "    x = GlobalMaxPooling1D()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    preds = Dense(20, activation='softmax')(x)\n",
        "\n",
        "    model = Model(sequence_input, preds)\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def create_callbacks(name):\n",
        "    tensorboard_callback = TensorBoard(log_dir=os.path.join(os.getcwd(), \"tb_log_newsgroups\", name),\n",
        "                                       write_graph=True,\n",
        "                                       write_grads=False,\n",
        "                                       embeddings_freq=1,\n",
        "                                       embeddings_layer_names=\"embedding\")\n",
        "    checkpoint_callback = ModelCheckpoint(filepath=\"./model-weights\" + name + \".{epoch:02d}-{val_loss:.6f}.hdf5\",\n",
        "                                          monitor='val_loss', verbose=0, save_best_only=True)\n",
        "    return [tensorboard_callback, checkpoint_callback]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUDCb3K6oZ7M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "d461be39-c757-415e-8881-4a9b26c7acbb"
      },
      "source": [
        "!head 20_newsgroup/alt.atheism/49960"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Xref: cantaloupe.srv.cs.cmu.edu alt.atheism:49960 alt.atheism.moderated:713 news.answers:7054 alt.answers:126\n",
            "Path: cantaloupe.srv.cs.cmu.edu!crabapple.srv.cs.cmu.edu!bb3.andrew.cmu.edu!news.sei.cmu.edu!cis.ohio-state.edu!magnus.acs.ohio-state.edu!usenet.ins.cwru.edu!agate!spool.mu.edu!uunet!pipex!ibmpcug!mantis!mathew\n",
            "From: mathew <mathew@mantis.co.uk>\n",
            "Newsgroups: alt.atheism,alt.atheism.moderated,news.answers,alt.answers\n",
            "Subject: Alt.Atheism FAQ: Atheist Resources\n",
            "Summary: Books, addresses, music -- anything related to atheism\n",
            "Keywords: FAQ, atheism, books, music, fiction, addresses, contacts\n",
            "Message-ID: <19930329115719@mantis.co.uk>\n",
            "Date: Mon, 29 Mar 1993 11:57:19 GMT\n",
            "Expires: Thu, 29 Apr 1993 11:57:19 GMT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1vN_T6huLkg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists(\"./model-weights\"):\n",
        "    os.makedirs(\"./model-weights\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Oi4ZYwOoWIq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "25417e72-e0c3-4b3d-cf0b-d8c06d5d18e2"
      },
      "source": [
        "# def main():\n",
        "BASE_DIR = './'\n",
        "glove_dir = os.path.join(BASE_DIR)\n",
        "text_data_dir = os.path.join(BASE_DIR, '20_newsgroup')\n",
        "embeddings_index = load_word_vectors(glove_dir)\n",
        "\n",
        "data = load_data(text_data_dir, vocab_size=20000, sequence_length=1000)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Indexing word vectors.\n",
            "Found 400000 word vectors.\n",
            "Processing text dataset\n",
            "Found 19997 texts.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQFgqs6coGjr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        },
        "outputId": "4f2eef7d-eca0-430c-91fd-d2050012efb6"
      },
      "source": [
        "data = tokenize_text(data)\n",
        "data = train_val_test_split(data)\n",
        "data[\"embedding_dim\"] = 100\n",
        "data[\"embedding_matrix\"] = embedding_index_to_matrix(embeddings_index=embeddings_index,\n",
        "                                                     vocab_size=data[\"vocab_size\"],\n",
        "                                                     embedding_dim=data[\"embedding_dim\"],\n",
        "                                                     word_index=data[\"tokenizer\"].word_index)\n",
        "\n",
        "# callbacks = create_callbacks(\"newsgroups-pretrained\")\n",
        "model = build_model(vocab_size=data[\"vocab_size\"],\n",
        "                    embedding_dim=data['embedding_dim'],\n",
        "                    sequence_length=data['sequence_length'],\n",
        "                    embedding_matrix=data['embedding_matrix'])\n",
        "\n",
        "model.fit(data[\"X_train\"], data[\"y_train\"],\n",
        "          batch_size=128,\n",
        "          epochs=10,\n",
        "          validation_data=(data[\"X_val\"], data[\"y_val\"]))\n",
        "#           callbacks=callbacks)\n",
        "\n",
        "model.save(\"newsgroup_model_word_embedding.h5\")\n",
        "\n",
        "score, acc = model.evaluate(x=data[\"X_test\"],\n",
        "                            y=data[\"y_test\"],\n",
        "                            batch_size=128)\n",
        "print('Test loss:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 174074 unique tokens.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0726 23:37:10.425402 140135493072768 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Shape of data tensor: (19997, 1000)\n",
            "Shape of label tensor: (19997, 20)\n",
            "Preparing embedding matrix.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0726 23:37:10.460755 140135493072768 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0726 23:37:10.469526 140135493072768 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0726 23:37:10.481250 140135493072768 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0726 23:37:10.482122 140135493072768 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0726 23:37:13.451493 140135493072768 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0726 23:37:13.511507 140135493072768 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0726 23:37:13.617422 140135493072768 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 15997 samples, validate on 3000 samples\n",
            "Epoch 1/10\n",
            "15997/15997 [==============================] - 8s 486us/step - loss: 2.2893 - acc: 0.2327 - val_loss: 1.6766 - val_acc: 0.4103\n",
            "Epoch 2/10\n",
            "15997/15997 [==============================] - 3s 191us/step - loss: 1.4004 - acc: 0.5040 - val_loss: 1.2721 - val_acc: 0.5567\n",
            "Epoch 3/10\n",
            "15997/15997 [==============================] - 3s 191us/step - loss: 1.0885 - acc: 0.6231 - val_loss: 1.1032 - val_acc: 0.6303\n",
            "Epoch 4/10\n",
            "15997/15997 [==============================] - 3s 191us/step - loss: 0.8931 - acc: 0.6982 - val_loss: 1.0230 - val_acc: 0.6550\n",
            "Epoch 5/10\n",
            "15997/15997 [==============================] - 3s 192us/step - loss: 0.7373 - acc: 0.7505 - val_loss: 0.9665 - val_acc: 0.6860\n",
            "Epoch 6/10\n",
            "15997/15997 [==============================] - 3s 192us/step - loss: 0.6135 - acc: 0.7903 - val_loss: 0.9354 - val_acc: 0.7033\n",
            "Epoch 7/10\n",
            "15997/15997 [==============================] - 3s 193us/step - loss: 0.4992 - acc: 0.8301 - val_loss: 0.9035 - val_acc: 0.7133\n",
            "Epoch 8/10\n",
            "15997/15997 [==============================] - 3s 193us/step - loss: 0.3899 - acc: 0.8698 - val_loss: 0.9904 - val_acc: 0.7040\n",
            "Epoch 9/10\n",
            "15997/15997 [==============================] - 3s 193us/step - loss: 0.3417 - acc: 0.8859 - val_loss: 0.9627 - val_acc: 0.7233\n",
            "Epoch 10/10\n",
            "15997/15997 [==============================] - 3s 193us/step - loss: 0.2556 - acc: 0.9149 - val_loss: 0.9904 - val_acc: 0.7290\n",
            "1000/1000 [==============================] - 0s 149us/step\n",
            "Test loss: 0.9039186096191406\n",
            "Test accuracy: 0.7259999990463257\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}